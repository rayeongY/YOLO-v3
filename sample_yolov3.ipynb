{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 217,
   "source": [
    "import os\r\n",
    "import cv2\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "import torch\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "\r\n",
    "# from data.yolo_dataset import YoloDataset, collate_fn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "source": [
    "from torch import nn\r\n",
    "\r\n",
    "\r\n",
    "class YOLOv3Loss(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super().__init__()\r\n",
    "        self.mse = nn.MSELoss(reduction='none')\r\n",
    "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\r\n",
    "        self.multiMargin = nn.MultiLabelSoftMarginLoss(reduction='none')        ## https://cvml.tistory.com/26\r\n",
    "                                                                \r\n",
    "\r\n",
    "        \r\n",
    "    def forward(self, pred, target, scale, anchors):\r\n",
    "\r\n",
    "        pred = pred.reshape(-1, 3, 85, scale, scale)\r\n",
    "        pred = pred.permute(0, 1, 3, 4, 2)\r\n",
    "        print(pred.shape)\r\n",
    "        print(target.shape)\r\n",
    "\r\n",
    "        ## no_obj_loss(No Object Loss):     Loss for objectness score      of non-object-assigned BBOXes\r\n",
    "        ## is_obj_loss(Object Loss):        Loss for objectness score      of     object-assigned BBOXes\r\n",
    "        ## coord_loss(Coordinates Loss):    Loss for predicted coordinates of     object-assigned BBOXes\r\n",
    "        ## class_loss(Classification Loss): Loss for predicted class-ids   of     object-assigned BBOXes \r\n",
    "\r\n",
    "        is_assigned = pred[..., 4] == 1     ## tensor([(element == 1) for element in 4th column of pred])   ## e.g. tensor([True, False, False, ...])\r\n",
    "        no_assigned = pred[..., 4] == 0     ## If use these boolean-list tensor as a indices,\r\n",
    "                                            ##    we can extract the only rows from target(label) tensor -- whose 4th column element(objectness score) is 1-or-0\r\n",
    "\r\n",
    "        print(pred[..., 4][no_assigned].shape)\r\n",
    "        print(target[..., 4][no_assigned.shape])\r\n",
    "\r\n",
    "        no_obj_loss = self.get_loss(pred[...,  4][no_assigned], target[...,  4][no_assigned], anchors, opt=\"NO_OBJ\")\r\n",
    "        is_obj_loss = self.get_loss(pred[...,  4][is_assigned], target[...,  4][is_assigned], anchors, opt=\"IS_OBJ\")\r\n",
    "        coord_loss =  self.get_loss(pred[..., :4][is_assigned], target[..., :4][is_assigned], anchors, opt=\"COORD\")\r\n",
    "        class_loss =  self.get_loss(pred[..., 5:][is_assigned], target[..., 5:][is_assigned], anchors, opt=\"CLASS\")\r\n",
    "        \r\n",
    "        loss = no_obj_loss + is_obj_loss + coord_loss + class_loss\r\n",
    "        return loss\r\n",
    "\r\n",
    "\r\n",
    "    def get_loss(self, pred, target, anchors, opt):\r\n",
    "        \r\n",
    "        if opt == \"NO_OBJ\":\r\n",
    "            loss = self.bce(pred, target)\r\n",
    "            return loss\r\n",
    "\r\n",
    "        elif opt == \"IS_OBJ\":\r\n",
    "            loss = self.bce(torch.sigmoid(pred), target)            ## If use [wh_IOU * target] instead of [target], MSE loss is better . . . maybe.\r\n",
    "            return loss                                             ##    cause [target] and [wh_IOU * target] values differ in \"Discrete\"/\"Continuous\"\r\n",
    "\r\n",
    "        elif opt == \"COORD\":\r\n",
    "            pred_bboxes =   torch.cat([torch.sigmoid(pred[..., 0:2]), torch.exp(pred[..., 2:4])          ], dim=1)\r\n",
    "            target_bboxes = torch.cat([              pred[..., 0:2] ,          (pred[..., 2:4] / anchors)], dim=1)\r\n",
    "            loss = self.mse(pred_bboxes, target_bboxes)\r\n",
    "            return loss\r\n",
    "\r\n",
    "        elif opt == \"CLASS\":\r\n",
    "            loss = self.multiMargin(pred, target)\r\n",
    "            return loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "source": [
    "from common.sampler import sampler\r\n",
    "from common.utils import *\r\n",
    "\r\n",
    "from torch.utils.data.dataset import Dataset\r\n",
    "\r\n",
    "from PIL import Image\r\n",
    "import torchvision\r\n",
    "\r\n",
    "class YoloDataset(Dataset):\r\n",
    "    def __init__(self,\r\n",
    "                 dataset_option,\r\n",
    "                 model_option,\r\n",
    "                 split=\"train\"):\r\n",
    "\r\n",
    "        self.dataset_option = dataset_option\r\n",
    "        self.model_option = model_option\r\n",
    "        self.classes = self.dataset_option[\"DATASET\"][\"CLASSES\"]\r\n",
    "\r\n",
    "        \r\n",
    "        dataset_name = dataset_option[\"DATASET\"][\"NAME\"]\r\n",
    "\r\n",
    "        assert split == \"train\" or split == \"valid\"\r\n",
    "        \r\n",
    "        assert dataset_name in [\"ship\", \"yolo-dataset\"]\r\n",
    "                \r\n",
    "        if dataset_name == \"yolo-dataset\" or dataset_name == \"ship\":\r\n",
    "            if split == \"train\":\r\n",
    "                dataset_type = \"train\"\r\n",
    "            elif split == \"valid\":\r\n",
    "                dataset_type = \"valid\"\r\n",
    "\r\n",
    "        root = self.dataset_option[\"DATASET\"][\"ROOT\"]\r\n",
    "        self.split = split\r\n",
    "        \r\n",
    "        self.dataset = self.load_dataset(os.path.join(root, dataset_type))\r\n",
    "\r\n",
    "\r\n",
    "    def __getitem__(self, idx):\r\n",
    "        img_path, label_path = self.dataset[idx]\r\n",
    "\r\n",
    "        ## load img\r\n",
    "        img_file = Image.open(img_path)\r\n",
    "        t = torchvision.transforms.Compose([torchvision.transforms.Resize((608, 608)), torchvision.transforms.ToTensor()])\r\n",
    "\r\n",
    "        img_file = t(img_file)\r\n",
    "\r\n",
    "        ## load label\r\n",
    "        label_f = open(label_path, \"r\")\r\n",
    "\r\n",
    "        labels = np.zeros((0, 5))\r\n",
    "        if os.fstat(label_f.fileno()).st_size:\r\n",
    "            labels = np.loadtxt(label_f, dtype=\"float\")\r\n",
    "            labels = labels.reshape(-1, 5)\r\n",
    "\r\n",
    "        #############################################################################################\r\n",
    "        ## transform \"labels\" - from np.shape(_, 5) to tensor(#ofS=3, A=3, S, S, 5 + class_offset) ##\r\n",
    "        ## i.e., (1) add objectness score, (2) apply one-hot encoding to object ids                ##\r\n",
    "        #############################################################################################\r\n",
    "        num_anchors = self.model_option[\"YOLOv3\"][\"NUM_ANCHORS\"]\r\n",
    "        anchors = self.model_option[\"YOLOv3\"][\"ANCHORS\"]\r\n",
    "        scales = self.model_option[\"YOLOv3\"][\"SCALES\"]\r\n",
    "        class_offset = 80\r\n",
    "        # class_offset = self.dataset_option[\"DATASET\"][\"NUM_CLASSES\"]\r\n",
    "\r\n",
    "        ##           tensor([# of S]=3,  [# of A]=3,     S,     S, 5 + class_offset)\r\n",
    "        label_maps = [torch.zeros((num_anchors // 3, scale, scale, 5 + class_offset)) for scale in scales]\r\n",
    "        for label in labels:\r\n",
    "            obj_ids, gtBBOX = label[0], label[1:5]\r\n",
    "            bx, by, bw, bh = gtBBOX\r\n",
    "            \r\n",
    "            ## (2) Create one-hot vector with list of object ids\r\n",
    "            obj_vec = [0] * class_offset\r\n",
    "            obj_vec[int(obj_ids)] = 1\r\n",
    "            # for obj_id in obj_ids:\r\n",
    "            #     obj_vec[int(obj_id)] = 1\r\n",
    "\r\n",
    "            ## (1) Set objectness score\r\n",
    "            ## . . . . before then, we should find (the correct cell_offset(Si: cy, Sj: cx) & the best-fitted anchor(Ai: pw, ph))\r\n",
    "            ## . . . .                          -- where g.t. bbox(from label) be assigned\r\n",
    "            ## . . . . => label_maps[idx of Scale: anchor assigned][idx of Anchor, Si, Sj, 4] = 1 ----- case of Best\r\n",
    "            ## . . . . => label_maps[idx of Scale: anchor assigned][idx of Anchor, Si, Sj, 4] = -1 ---- case of Non-best (to be ignored)\r\n",
    "            ## . . . . => DEFAULT = 0 ----------------------------------------------------------------- case of No-assigned\r\n",
    "            ## \r\n",
    "            ## . . (1-1) How evaluate the \"goodness\" of anchor box\r\n",
    "            ## . . . . .     is to compare \"Similarity\" between the anchor box and g.t. BBOX\r\n",
    "            ## . . . . . => Calculate \"width-and-height-based IOU\" between anchBOX and gtBBOX\r\n",
    "            ## . . . . . => Pick the anchBOX in descending order with whIOU value\r\n",
    "            anchors_wh = torch.tensor(anchors).to(device='cpu').reshape(-1, 2)         ## (3, 3, 2) -> (9, 2)\r\n",
    "            gtBBOX_wh = torch.tensor(gtBBOX[2:4]).to(device='cpu')\r\n",
    "            wh_IOUs = width_height_IOU(anchors_wh, gtBBOX_wh)\r\n",
    "\r\n",
    "            anchor_indices = wh_IOUs.argsort(descending=True, dim=0)\r\n",
    "\r\n",
    "            ## Flag list for checking whether other anchor has been already picked in the scale\r\n",
    "            is_scale_occupied = [False] * 3\r\n",
    "\r\n",
    "            for anchor_index in anchor_indices:\r\n",
    "\r\n",
    "                ## To mark the anchor\r\n",
    "                ## . . (1) Get information of the anchor BBOX\r\n",
    "                scale_idx = torch.div(anchor_index, len(scales), rounding_mode='floor')\r\n",
    "                anch_idx_in_scale = anchor_index % len(scales)\r\n",
    "\r\n",
    "                ## . . (2) then, Get cell information(Si: cy, Sj: cx) of the g.t.BBOX\r\n",
    "                scale = scales[scale_idx]\r\n",
    "                cx = int(bx * scale)          ## .....??\r\n",
    "                cy = int(by * scale)\r\n",
    "                gt_tx = bx * scale - cx\r\n",
    "                gt_ty = by * scale - cy\r\n",
    "                gtBBOX[0:2] = gt_tx, gt_ty\r\n",
    "\r\n",
    "                ## Get record of the cell information in the scale\r\n",
    "                ## . . to avoid overlapping bboxes\r\n",
    "                is_cell_occupied = label_maps[scale_idx][anch_idx_in_scale, cy, cx,  4]\r\n",
    "\r\n",
    "                if not is_cell_occupied and not is_scale_occupied[scale_idx]:       ## if there is no other overlapping-liked bbox and I'm the best\r\n",
    "                    label_maps[scale_idx][anch_idx_in_scale, cy, cx,  4] = 1\r\n",
    "                    label_maps[scale_idx][anch_idx_in_scale, cy, cx, :4] = torch.tensor(gtBBOX)\r\n",
    "                    label_maps[scale_idx][anch_idx_in_scale, cy, cx, 5:] = torch.tensor(obj_ids)\r\n",
    "                    is_scale_occupied[scale_idx] = True                             ## the best-fitted anchor has been picked in this scale\r\n",
    "                \r\n",
    "                elif wh_IOUs[anchor_index] > 0.5:\r\n",
    "                    label_maps[scale_idx][anch_idx_in_scale, cy, cx,  4] = -1        ## this anchor is not the best, so we will ignore it\r\n",
    "\r\n",
    "        return img_file, label_maps, img_path\r\n",
    "\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.dataset)\r\n",
    "\r\n",
    "\r\n",
    "    # def load_dataset(self, f_list_path):\r\n",
    "    def load_dataset(self, dataset_path):\r\n",
    "        image_set = []\r\n",
    "\r\n",
    "        for r, _, f in os.walk(dataset_path):\r\n",
    "            for file in f:\r\n",
    "                if file.lower().endswith((\".png\", \".jpg\", \".bmp\", \".jpeg\")):\r\n",
    "                    # set paths - both image and label file\r\n",
    "                    img_path = os.path.join(r, file).replace(os.sep, '/')\r\n",
    "                    label_path = os.path.splitext(img_path)[0] + \".txt\"\r\n",
    "\r\n",
    "                    if not os.path.isfile(img_path) or not os.path.isfile(label_path):\r\n",
    "                        continue\r\n",
    "                                \r\n",
    "                    image_set.append((img_path, label_path))\r\n",
    "                \r\n",
    "        return image_set\r\n",
    "    \r\n",
    "\r\n",
    "def collate_fn(batch):\r\n",
    "        img_files = []\r\n",
    "        labels_1 = []\r\n",
    "        labels_2 = []\r\n",
    "        labels_3 = []\r\n",
    "        img_paths = []\r\n",
    "\r\n",
    "        for b in batch:\r\n",
    "            img_files.append(b[0])\r\n",
    "\r\n",
    "            labels_1.append(b[1][0])\r\n",
    "            labels_2.append(b[1][1])\r\n",
    "            labels_3.append(b[1][2])\r\n",
    "\r\n",
    "            img_paths.append(b[2])\r\n",
    "\r\n",
    "        img_files = torch.stack(img_files, 0)\r\n",
    "\r\n",
    "        labels_1 = torch.stack(labels_1, 0)\r\n",
    "        labels_2 = torch.stack(labels_2, 0)\r\n",
    "        labels_3 = torch.stack(labels_3, 0)\r\n",
    "        img_labels = (labels_1, labels_2, labels_3)\r\n",
    "\r\n",
    "        return img_files, img_labels, img_paths"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "source": [
    "def valid(\r\n",
    "    model,\r\n",
    "    valid_loader,\r\n",
    "    model_option,\r\n",
    "    epoch,\r\n",
    "    # anchors,\r\n",
    "    ):\r\n",
    "    model.eval()\r\n",
    "    true_pred_num = 0\r\n",
    "    gt_num = 0\r\n",
    "\r\n",
    "    for i, batch_img, batch_label, batch_img_path in enumerate(valid_loader, 0):\r\n",
    "\r\n",
    "        pred = model(batch_img)\r\n",
    "\r\n",
    "        ## Post-Processing?\r\n",
    "\r\n",
    "        ## Get the number of both true predictions and ground truth\r\n",
    "\r\n",
    "\r\n",
    "    ## Examine Accuracy\r\n",
    "    acc = (true_pred_num / gt_num + 1e-16) * 100\r\n",
    "    \r\n",
    "    return acc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "source": [
    "dataset_option = {  \"DATASET\": {\r\n",
    "                        \"NAME\": \"ship\",\r\n",
    "                        \"ROOT\": \"C:/Users/ryyoon/MA_MSS/ship-tracking/datasets/ship\",\r\n",
    "                        \"CLASSES\": {\r\n",
    "                            #    \"선박\": 0, \"부표\": 1, \"어망부표\": 2,\r\n",
    "                            #    \"해상풍력\": 3, \"등대\": 4, \"기타부유물\" : 5\r\n",
    "                               \"선박\": 0, \"부표\": 1, \"어망부표\": 1,\r\n",
    "                               \"해상풍력\": 1, \"등대\": 1, \"기타부유물\" : 1\r\n",
    "                        },\r\n",
    "                        \"NUM_CLASSES\": 2\r\n",
    "                     }\r\n",
    "                 }\r\n",
    "\r\n",
    "model_option = {\"YOLOv3\": {\r\n",
    "                     \"SCALES\": [608 // 32, 608 // 16, 608 // 8],\r\n",
    "                     \"NUM_ANCHORS\": 9,\r\n",
    "                     \"ANCHORS\": [[( 12,  16), ( 19,  36), ( 40,  28)],\r\n",
    "                                 [( 36,  75), ( 76,  55), ( 72, 146)],\r\n",
    "                                 [(142, 110), (192, 243), (459, 401)]],\r\n",
    "                    #  \"ANCHORS\": [[( 10, 13), ( 16,  30), ( 33,  23)],\r\n",
    "                    #              [( 30, 61), ( 62,  45), ( 59, 119)],\r\n",
    "                    #              [(116, 90), (156, 198), (373, 326)]]\r\n",
    "                    }\r\n",
    "               }\r\n",
    "\r\n",
    "optim_option = {\"OPTIMIZER\": {\r\n",
    "                     \"METHOD\": \"adam\",\r\n",
    "                     \"BATCH_SIZE\": 4,\r\n",
    "                     \"EPOCHS\": 1,\r\n",
    "                     \"LR\": 1e-4,\r\n",
    "                    }\r\n",
    "               }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "source": [
    "epochs = optim_option[\"OPTIMIZER\"][\"EPOCHS\"]\r\n",
    "batch_size = optim_option[\"OPTIMIZER\"][\"BATCH_SIZE\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "source": [
    "from darknet2pytorch import DarknetParser\r\n",
    "\r\n",
    "cfg = './weights/darknet/yolov4.cfg'\r\n",
    "weight = './weights/darknet/yolov4.weights'\r\n",
    "model = DarknetParser(cfg, weight)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "parse from './weights/darknet/yolov4.cfg'\n",
      "done\n",
      "\n",
      "load weights from : './weights/darknet/yolov4.weights'\n",
      "Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "0 convolutional load weights : [0.004]/[245.779] mb\n",
      "Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "1 convolutional load weights : [0.075]/[245.779] mb\n",
      "Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "2 convolutional load weights : [0.092]/[245.779] mb\n",
      "3 route        load weights : [0.092]/[245.779] mb\n",
      "Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "4 convolutional load weights : [0.108]/[245.779] mb\n",
      "Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "5 convolutional load weights : [0.117]/[245.779] mb\n",
      "Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "6 convolutional load weights : [0.188]/[245.779] mb\n",
      "7 shortcut     load weights : [0.188]/[245.779] mb\n",
      "Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "8 convolutional load weights : [0.204]/[245.779] mb\n",
      "9 route        load weights : [0.204]/[245.779] mb\n",
      "Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "10 convolutional load weights : [0.237]/[245.779] mb\n",
      "Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "11 convolutional load weights : [0.520]/[245.779] mb\n",
      "Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "12 convolutional load weights : [0.552]/[245.779] mb\n",
      "13 route        load weights : [0.552]/[245.779] mb\n",
      "Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "14 convolutional load weights : [0.584]/[245.779] mb\n",
      "Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "15 convolutional load weights : [0.601]/[245.779] mb\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "16 convolutional load weights : [0.743]/[245.779] mb\n",
      "17 shortcut     load weights : [0.743]/[245.779] mb\n",
      "Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "18 convolutional load weights : [0.759]/[245.779] mb\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "19 convolutional load weights : [0.901]/[245.779] mb\n",
      "20 shortcut     load weights : [0.901]/[245.779] mb\n",
      "Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "21 convolutional load weights : [0.917]/[245.779] mb\n",
      "22 route        load weights : [0.917]/[245.779] mb\n",
      "Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "23 convolutional load weights : [0.982]/[245.779] mb\n",
      "Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "24 convolutional load weights : [2.111]/[245.779] mb\n",
      "Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "25 convolutional load weights : [2.238]/[245.779] mb\n",
      "26 route        load weights : [2.238]/[245.779] mb\n",
      "Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "27 convolutional load weights : [2.365]/[245.779] mb\n",
      "Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "28 convolutional load weights : [2.429]/[245.779] mb\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "29 convolutional load weights : [2.994]/[245.779] mb\n",
      "30 shortcut     load weights : [2.994]/[245.779] mb\n",
      "Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "31 convolutional load weights : [3.058]/[245.779] mb\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "32 convolutional load weights : [3.622]/[245.779] mb\n",
      "33 shortcut     load weights : [3.622]/[245.779] mb\n",
      "Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "34 convolutional load weights : [3.687]/[245.779] mb\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "35 convolutional load weights : [4.251]/[245.779] mb\n",
      "36 shortcut     load weights : [4.251]/[245.779] mb\n",
      "Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "37 convolutional load weights : [4.316]/[245.779] mb\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "38 convolutional load weights : [4.880]/[245.779] mb\n",
      "39 shortcut     load weights : [4.880]/[245.779] mb\n",
      "Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "40 convolutional load weights : [4.945]/[245.779] mb\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "41 convolutional load weights : [5.509]/[245.779] mb\n",
      "42 shortcut     load weights : [5.509]/[245.779] mb\n",
      "Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "43 convolutional load weights : [5.574]/[245.779] mb\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "44 convolutional load weights : [6.138]/[245.779] mb\n",
      "45 shortcut     load weights : [6.138]/[245.779] mb\n",
      "Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "46 convolutional load weights : [6.203]/[245.779] mb\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "47 convolutional load weights : [6.767]/[245.779] mb\n",
      "48 shortcut     load weights : [6.767]/[245.779] mb\n",
      "Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "49 convolutional load weights : [6.831]/[245.779] mb\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "50 convolutional load weights : [7.396]/[245.779] mb\n",
      "51 shortcut     load weights : [7.396]/[245.779] mb\n",
      "Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "52 convolutional load weights : [7.460]/[245.779] mb\n",
      "53 route        load weights : [7.460]/[245.779] mb\n",
      "Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "54 convolutional load weights : [7.714]/[245.779] mb\n",
      "Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "55 convolutional load weights : [12.222]/[245.779] mb\n",
      "Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "56 convolutional load weights : [12.726]/[245.779] mb\n",
      "57 route        load weights : [12.726]/[245.779] mb\n",
      "Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "58 convolutional load weights : [13.230]/[245.779] mb\n",
      "Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "59 convolutional load weights : [13.484]/[245.779] mb\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "60 convolutional load weights : [15.738]/[245.779] mb\n",
      "61 shortcut     load weights : [15.738]/[245.779] mb\n",
      "Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "62 convolutional load weights : [15.992]/[245.779] mb\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "63 convolutional load weights : [18.245]/[245.779] mb\n",
      "64 shortcut     load weights : [18.245]/[245.779] mb\n",
      "Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "65 convolutional load weights : [18.499]/[245.779] mb\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "66 convolutional load weights : [20.753]/[245.779] mb\n",
      "67 shortcut     load weights : [20.753]/[245.779] mb\n",
      "Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "68 convolutional load weights : [21.007]/[245.779] mb\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "69 convolutional load weights : [23.261]/[245.779] mb\n",
      "70 shortcut     load weights : [23.261]/[245.779] mb\n",
      "Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "71 convolutional load weights : [23.515]/[245.779] mb\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "72 convolutional load weights : [25.769]/[245.779] mb\n",
      "73 shortcut     load weights : [25.769]/[245.779] mb\n",
      "Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "74 convolutional load weights : [26.023]/[245.779] mb\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "75 convolutional load weights : [28.277]/[245.779] mb\n",
      "76 shortcut     load weights : [28.277]/[245.779] mb\n",
      "Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "77 convolutional load weights : [28.531]/[245.779] mb\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "78 convolutional load weights : [30.785]/[245.779] mb\n",
      "79 shortcut     load weights : [30.785]/[245.779] mb\n",
      "Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "80 convolutional load weights : [31.038]/[245.779] mb\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "81 convolutional load weights : [33.292]/[245.779] mb\n",
      "82 shortcut     load weights : [33.292]/[245.779] mb\n",
      "Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "83 convolutional load weights : [33.546]/[245.779] mb\n",
      "84 route        load weights : [33.546]/[245.779] mb\n",
      "Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "85 convolutional load weights : [34.554]/[245.779] mb\n",
      "Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "86 convolutional load weights : [52.570]/[245.779] mb\n",
      "Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "87 convolutional load weights : [54.578]/[245.779] mb\n",
      "88 route        load weights : [54.578]/[245.779] mb\n",
      "Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "89 convolutional load weights : [56.585]/[245.779] mb\n",
      "Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "90 convolutional load weights : [57.593]/[245.779] mb\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "91 convolutional load weights : [66.601]/[245.779] mb\n",
      "92 shortcut     load weights : [66.601]/[245.779] mb\n",
      "Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "93 convolutional load weights : [67.609]/[245.779] mb\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "94 convolutional load weights : [76.617]/[245.779] mb\n",
      "95 shortcut     load weights : [76.617]/[245.779] mb\n",
      "Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "96 convolutional load weights : [77.624]/[245.779] mb\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "97 convolutional load weights : [86.632]/[245.779] mb\n",
      "98 shortcut     load weights : [86.632]/[245.779] mb\n",
      "Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "99 convolutional load weights : [87.640]/[245.779] mb\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "100 convolutional load weights : [96.648]/[245.779] mb\n",
      "101 shortcut     load weights : [96.648]/[245.779] mb\n",
      "Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "102 convolutional load weights : [97.656]/[245.779] mb\n",
      "103 route        load weights : [97.656]/[245.779] mb\n",
      "Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "104 convolutional load weights : [101.671]/[245.779] mb\n",
      "Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "105 convolutional load weights : [103.679]/[245.779] mb\n",
      "Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "106 convolutional load weights : [121.695]/[245.779] mb\n",
      "Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "107 convolutional load weights : [123.703]/[245.779] mb\n",
      "108 maxpool      load weights : [123.703]/[245.779] mb\n",
      "109 route        load weights : [123.703]/[245.779] mb\n",
      "110 maxpool      load weights : [123.703]/[245.779] mb\n",
      "111 route        load weights : [123.703]/[245.779] mb\n",
      "112 maxpool      load weights : [123.703]/[245.779] mb\n",
      "113 route        load weights : [123.703]/[245.779] mb\n",
      "Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "114 convolutional load weights : [127.710]/[245.779] mb\n",
      "Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "115 convolutional load weights : [145.726]/[245.779] mb\n",
      "Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "116 convolutional load weights : [147.734]/[245.779] mb\n",
      "Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "117 convolutional load weights : [148.238]/[245.779] mb\n",
      "118 upsample     load weights : [148.238]/[245.779] mb\n",
      "119 route        load weights : [148.238]/[245.779] mb\n",
      "Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "120 convolutional load weights : [148.742]/[245.779] mb\n",
      "121 route        load weights : [148.742]/[245.779] mb\n",
      "Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "122 convolutional load weights : [149.245]/[245.779] mb\n",
      "Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "123 convolutional load weights : [153.753]/[245.779] mb\n",
      "Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "124 convolutional load weights : [154.257]/[245.779] mb\n",
      "Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "125 convolutional load weights : [158.765]/[245.779] mb\n",
      "Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "126 convolutional load weights : [159.269]/[245.779] mb\n",
      "Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "127 convolutional load weights : [159.396]/[245.779] mb\n",
      "128 upsample     load weights : [159.396]/[245.779] mb\n",
      "129 route        load weights : [159.396]/[245.779] mb\n",
      "Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "130 convolutional load weights : [159.523]/[245.779] mb\n",
      "131 route        load weights : [159.523]/[245.779] mb\n",
      "Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "132 convolutional load weights : [159.650]/[245.779] mb\n",
      "Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "133 convolutional load weights : [160.779]/[245.779] mb\n",
      "Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "134 convolutional load weights : [160.906]/[245.779] mb\n",
      "Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "135 convolutional load weights : [162.035]/[245.779] mb\n",
      "Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "136 convolutional load weights : [162.161]/[245.779] mb\n",
      "Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "137 convolutional load weights : [163.290]/[245.779] mb\n",
      "Conv2d(256, 255, kernel_size=(1, 1), stride=(1, 1))\n",
      "138 convolutional load weights : [163.540]/[245.779] mb\n",
      "139 yolo         load weights : [163.540]/[245.779] mb\n",
      "140 route        load weights : [163.540]/[245.779] mb\n",
      "Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "141 convolutional load weights : [164.669]/[245.779] mb\n",
      "142 route        load weights : [164.669]/[245.779] mb\n",
      "Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "143 convolutional load weights : [165.173]/[245.779] mb\n",
      "Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "144 convolutional load weights : [169.681]/[245.779] mb\n",
      "Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "145 convolutional load weights : [170.185]/[245.779] mb\n",
      "Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "146 convolutional load weights : [174.693]/[245.779] mb\n",
      "Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "147 convolutional load weights : [175.197]/[245.779] mb\n",
      "Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "148 convolutional load weights : [179.704]/[245.779] mb\n",
      "Conv2d(512, 255, kernel_size=(1, 1), stride=(1, 1))\n",
      "149 convolutional load weights : [180.203]/[245.779] mb\n",
      "150 yolo         load weights : [180.203]/[245.779] mb\n",
      "151 route        load weights : [180.203]/[245.779] mb\n",
      "Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "152 convolutional load weights : [184.711]/[245.779] mb\n",
      "153 route        load weights : [184.711]/[245.779] mb\n",
      "Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "154 convolutional load weights : [186.719]/[245.779] mb\n",
      "Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "155 convolutional load weights : [204.735]/[245.779] mb\n",
      "Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "156 convolutional load weights : [206.743]/[245.779] mb\n",
      "Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "157 convolutional load weights : [224.758]/[245.779] mb\n",
      "Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "158 convolutional load weights : [226.766]/[245.779] mb\n",
      "Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "159 convolutional load weights : [244.782]/[245.779] mb\n",
      "Conv2d(1024, 255, kernel_size=(1, 1), stride=(1, 1))\n",
      "160 convolutional load weights : [245.779]/[245.779] mb\n",
      "161 yolo         load weights : [245.779]/[245.779] mb\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "source": [
    "##############\r\n",
    "## DATALOAD ##\r\n",
    "##############\r\n",
    "train_dataset = YoloDataset(dataset_option, model_option, split=\"valid\")\r\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\r\n",
    "valid_dataset = YoloDataset(dataset_option, model_option, split=\"valid\")\r\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "source": [
    "loss_function = YOLOv3Loss()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "source": [
    "def train(\r\n",
    "        model,\r\n",
    "        train_loader,\r\n",
    "        loss_func,\r\n",
    "        dataset_option,\r\n",
    "        model_option,\r\n",
    "        epoch,\r\n",
    "        # anchors,\r\n",
    "        ):\r\n",
    "    model.train()\r\n",
    "\r\n",
    "    scales = torch.tensor(model_option[\"YOLOv3\"][\"SCALES\"]).to(device='cpu')       ## [13, 26, 52]\r\n",
    "    anchors = torch.tensor(model_option[\"YOLOv3\"][\"ANCHORS\"]).to(device='cpu')\r\n",
    "\r\n",
    "    for i, (batch_img, batch_label, batch_img_path) in enumerate(train_loader, 0):\r\n",
    "        # batch_size = batch_img.size(0)\r\n",
    "        \r\n",
    "        #################\r\n",
    "        ##  FORWARDING ##\r\n",
    "        #################\r\n",
    "        pred = model(batch_img)                                                      ### batch_img: tensor(   N, 3, 416, 416) . . . . . . . . . . . N = batch_size\r\n",
    "        loss = ( loss_func(pred[0], batch_label[0], scales[0], anchors=anchors[0])    ######## pred: tensor(3, N, 3, S, S, 1 + 4 + class_offset) . . S = scale_size\r\n",
    "               + loss_func(pred[1], batch_label[1], scales[1], anchors=anchors[1])    # batch_label: tensor(3, N, 3, S, S, 1 + 4 + class_offset)\r\n",
    "               + loss_func(pred[2], batch_label[2], scales[2], anchors=anchors[2]) )  ##### anchors: tensor(3,    3,       2) . . . is list of pairs(anch_w, anch_h)\r\n",
    "\r\n",
    "        #################\r\n",
    "        ## BACKWARDING ##\r\n",
    "        #################\r\n",
    "        loss.backward()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "source": [
    "for epoch in range(epochs):\r\n",
    "    ###########\r\n",
    "    ## TRAIN ##\r\n",
    "    ###########\r\n",
    "    train(  \r\n",
    "            model,\r\n",
    "            train_loader,\r\n",
    "            loss_function,\r\n",
    "            dataset_option,\r\n",
    "            model_option,\r\n",
    "            epoch,\r\n",
    "            # anchors,\r\n",
    "          )\r\n",
    "        \r\n",
    "    #######################\r\n",
    "    ## VALID (INFERENCE) ##\r\n",
    "    #######################\r\n",
    "    acc = valid(\r\n",
    "                 model,\r\n",
    "                 valid_loader,\r\n",
    "                 model_option,\r\n",
    "                 epoch,\r\n",
    "                 # anchors,\r\n",
    "               )\r\n",
    "\r\n",
    "    print(f\"Epoch: ({epoch + 1}/{epochs}) . . . [acc: {acc:.2f}]\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([64, 3, 19, 19, 85])\n",
      "torch.Size([4, 3, 19, 19, 90])\n",
      "torch.Size([0])\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "index 64 is out of bounds for dimension 0 with size 4",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-236-ddac90947532>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m## TRAIN ##\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m###########\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     train(  \n\u001b[0m\u001b[0;32m      6\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-235-ccdbf5fc4de8>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, loss_func, dataset_option, model_option, epoch)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m#################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_img\u001b[0m\u001b[1;33m)\u001b[0m                                                      \u001b[1;31m### batch_img: tensor(   N, 3, 416, 416) . . . . . . . . . . . N = batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         loss = ( loss_func(pred[0], batch_label[0], scales[0], anchors=anchors[0])    ######## pred: tensor(3, N, 3, S, S, 1 + 4 + class_offset) . . S = scale_size\n\u001b[0m\u001b[0;32m     23\u001b[0m                \u001b[1;33m+\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_label\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscales\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manchors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0manchors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;31m# batch_label: tensor(3, N, 3, S, S, 1 + 4 + class_offset)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                + loss_func(pred[2], batch_label[2], scales[2], anchors=anchors[2]) )  ##### anchors: tensor(3,    3,       2) . . . is list of pairs(anch_w, anch_h)\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-230-d24241be4a87>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, pred, target, scale, anchors)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mno_assigned\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mno_assigned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mno_obj_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mno_assigned\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mno_assigned\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manchors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"NO_OBJ\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 64 is out of bounds for dimension 0 with size 4"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit ('torch': conda)"
  },
  "interpreter": {
   "hash": "3cbf2cfe2af17255c7550a0a36495165331228ad52d7cf7dc2787e8b35bbde01"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}