{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "\r\n",
    "from common.parser import yaml_parser\r\n",
    "from common.recoder import save_checkpoint\r\n",
    "from data.yolo_dataset import *\r\n",
    "from model.MyYOLOv3 import YOLOv3Loss\r\n",
    "from model.darknet2pytorch import DarknetParser\r\n",
    "\r\n",
    "import torch\r\n",
    "import torch.nn\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import easydict\r\n",
    "\r\n",
    "args = easydict.EasyDict({\r\n",
    "    \"config\": \"./configs/darknet/yolov4.cfg\",\r\n",
    "    \"weight\": \"./configs/darknet/yolov4.weights\",\r\n",
    "    \"dataset\": \"./configs/dataset/yolo_dataset.yml\",\r\n",
    "    \"model\": \"./configs/model/yolo_model.yml\",\r\n",
    "    \"optimizer\": \"./configs/optimizer/optimizer.yml\",\r\n",
    "    \"weight_save_dir\": \"./weights\"\r\n",
    "})\r\n",
    "\r\n",
    "\r\n",
    "dataset_option = yaml_parser(args.dataset)\r\n",
    "model_option = yaml_parser(args.model)\r\n",
    "optimizer_option = yaml_parser(args.optimizer)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from model.darknet2pytorch import DarknetParser\r\n",
    "\r\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\r\n",
    "model = DarknetParser(args.config, args.weight).to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "loss_function = YOLOv3Loss()\r\n",
    "\r\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=optimizer_option[\"OPTIMIZER\"][\"LR\"])\r\n",
    "# optimizer_option[\"OPTIMIZER\"][\"ITERS_PER_EPOCH\"] = len(train_dataset) // optimizer_option[\"OPTIMIZER\"][\"BATCH_SIZE\"]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from PIL import Image\r\n",
    "\r\n",
    "root = \"C:/Users/ryyoon/MA_MSS/ship-tracking/datasets/ship/validation\"\r\n",
    "path = \"daecheon_20201113_0000_011.jpg\"\r\n",
    "\r\n",
    "img_path = os.path.join(root, path).replace(os.sep, \"/\")\r\n",
    "\r\n",
    "img = Image.open(img_path)\r\n",
    "\r\n",
    "import torchvision\r\n",
    "\r\n",
    "t = torchvision.transforms.Compose([torchvision.transforms.Resize((608, 608)), torchvision.transforms.ToTensor()])\r\n",
    "\r\n",
    "img = t(img).unsqueeze(0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "label_path = os.path.join(root, os.path.splitext(path)[0] + \".txt\").replace(os.sep, \"/\")\r\n",
    "f = open(label_path, \"r\")\r\n",
    "labels = np.zeros((0, 5))\r\n",
    "if os.fstat(f.fileno()).st_size:\r\n",
    "    labels = np.loadtxt(f, dtype=\"float\")\r\n",
    "    labels = labels.reshape(-1, 5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_anchors = model_option[\"YOLOv3\"][\"NUM_ANCHORS\"]\r\n",
    "anchors = model_option[\"YOLOv3\"][\"ANCHORS\"]\r\n",
    "scales = model_option[\"YOLOv3\"][\"SCALES\"]\r\n",
    "class_offset = 80"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "label_maps = [torch.zeros((num_anchors // 3, scale, scale, 5 + class_offset)) for scale in scales]\r\n",
    "for label in labels:\r\n",
    "    obj_ids, gtBBOX = label[0], label[1:5]\r\n",
    "    bx, by, bw, bh = gtBBOX\r\n",
    "    \r\n",
    "    obj_vec = [0] * class_offset\r\n",
    "    obj_vec[int(obj_ids)] = 1\r\n",
    "\r\n",
    "    anchors_wh = torch.tensor(anchors).reshape(-1, 2)         ## (3, 3, 2) -> (9, 2)\r\n",
    "    gtBBOX_wh = torch.tensor(gtBBOX[2:4])\r\n",
    "    wh_IOUs = width_height_IOU(anchors_wh, gtBBOX_wh)\r\n",
    "\r\n",
    "    anchor_indices = wh_IOUs.argsort(descending=True, dim=0)\r\n",
    "\r\n",
    "    is_scale_occupied = [False] * 3\r\n",
    "\r\n",
    "    for anchor_index in anchor_indices:\r\n",
    "\r\n",
    "        scale_idx = torch.div(anchor_index, len(scales), rounding_mode='floor')\r\n",
    "        anch_idx_in_scale = anchor_index % len(scales)\r\n",
    "\r\n",
    "        scale = scales[scale_idx]\r\n",
    "        cx = int(bx * scale)          ## .....??\r\n",
    "        cy = int(by * scale)\r\n",
    "        gt_tx = bx * scale - cx\r\n",
    "        gt_ty = by * scale - cy\r\n",
    "        gtBBOX[0:2] = gt_tx, gt_ty\r\n",
    "\r\n",
    "        is_cell_occupied = label_maps[scale_idx][anch_idx_in_scale, cy, cx,  4]\r\n",
    "\r\n",
    "        if not is_cell_occupied and not is_scale_occupied[scale_idx]:       ## if there is no other overlapping-liked bbox and I'm the best\r\n",
    "            label_maps[scale_idx][anch_idx_in_scale, cy, cx,  4] = 1\r\n",
    "            label_maps[scale_idx][anch_idx_in_scale, cy, cx, :4] = torch.tensor(gtBBOX)\r\n",
    "            label_maps[scale_idx][anch_idx_in_scale, cy, cx, 5:] = torch.tensor(obj_ids)\r\n",
    "            is_scale_occupied[scale_idx] = True                             ## the best-fitted anchor has been picked in this scale\r\n",
    "        \r\n",
    "        elif wh_IOUs[anchor_index] > 0.5:\r\n",
    "            label_maps[scale_idx][anch_idx_in_scale, cy, cx,  4] = -1        ## this anchor is not the best, so we will ignore it\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.train()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scales = torch.tensor(model_option[\"YOLOv3\"][\"SCALES\"]).to(device)       ## [13, 26, 52]\r\n",
    "anchors = torch.tensor(model_option[\"YOLOv3\"][\"ANCHORS\"]).to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "b_img = img.to(device)\r\n",
    "b_label = [label.to(device).unsqueeze(0) for label in label_maps]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pred = model(b_img)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(pred[0].shape, b_label[0].shape)\r\n",
    "print(pred[1].shape, b_label[1].shape)\r\n",
    "print(pred[2].shape, b_label[2].shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "loss = ( loss_function(pred[2], b_label[0], scales[0], anchors[0])\r\n",
    "        + loss_function(pred[1], b_label[1], scales[1], anchors[1])\r\n",
    "        + loss_function(pred[0], b_label[2], scales[2], anchors[2]) )\r\n",
    "\r\n",
    "loss /= 3\r\n",
    "\r\n",
    "print(loss.item())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def NMS(pred):\r\n",
    "    surpressed_pred = pred\r\n",
    "    \r\n",
    "    return surpressed_pred"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.eval()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "surpressed_pred = NMS(pred)\r\n",
    "\r\n",
    "print(surpressed_pred.shape)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit ('torch': conda)"
  },
  "interpreter": {
   "hash": "3cbf2cfe2af17255c7550a0a36495165331228ad52d7cf7dc2787e8b35bbde01"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}